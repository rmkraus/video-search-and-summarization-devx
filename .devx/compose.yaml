configs:
  server_config_dual_gpu:
    content: "summarization:\n  enable: true\n  method: \"batch\"\n  llm:\n    model:\
      \ meta/llama-3.1-8b-instruct\n    base_url: http://llama-3-1-8b-instruct:8000/v1\n\
      \    max_tokens: 2048\n    temperature: 0.2\n    top_p: 0.7\n  embedding:\n\
      \    model: \"nvidia/llama-3.2-nv-embedqa-1b-v2\"\n    base_url: http://llama-3-2-nv-embedqa:8000/v1\n\
      \  params:\n    batch_size: 5\n    batch_max_concurrency: 20\n  prompts:\n \
      \   caption: \"Write a concise and clear dense caption for the provided warehouse\
      \ video, focusing on irregular or hazardous events such as boxes falling, workers\
      \ not wearing PPE, workers falling, workers taking photographs, workers chitchatting,\
      \ forklift stuck, etc. Start and end each sentence with a time stamp.\"\n  \
      \  caption_summarization: \"You should summarize the following events of a warehouse\
      \ in the format start_time:end_time:caption. For start_time and end_time use\
      \ . to seperate seconds, minutes, hours. If during a time segment only regular\
      \ activities happen, then ignore them, else note any irregular activities in\
      \ detail. The output should be bullet points in the format start_time:end_time:\
      \ detailed_event_description. Don't return anything else except the bullet points.\"\
      \n    summary_aggregation: \"You are a warehouse monitoring system. Given the\
      \ caption in the form start_time:end_time: caption, Aggregate the following\
      \ captions in the format start_time:end_time:event_description. If the event_description\
      \ is the same as another event_description, aggregate the captions in the format\
      \ start_time1:end_time1,...,start_timek:end_timek:event_description. If any\
      \ two adjacent end_time1 and start_time2 is within a few tenths of a second,\
      \ merge the captions in the format start_time1:end_time2. The output should\
      \ only contain bullet points.  Cluster the output into Unsafe Behavior, Operational\
      \ Inefficiencies, Potential Equipment Damage and Unauthorized Personnel\"\n\n\
      chat:\n  rag: graph-rag # graph-rag or vector-rag #If using a small LLM model,\
      \ vector-rag is recommended.\n  params:\n    batch_size: 5\n    top_k: 5\n \
      \ llm:\n    model: meta/llama-3.1-8b-instruct\n    base_url: http://llama-3-1-8b-instruct:8000/v1\n\
      \    max_tokens: 2048\n    temperature: 0.2\n    top_p: 0.7\n  embedding:\n\
      \    model: \"nvidia/llama-3.2-nv-embedqa-1b-v2\"\n    base_url: http://llama-3-2-nv-embedqa:8000/v1\n\
      \  reranker:\n    model: \"nvidia/llama-3.2-nv-rerankqa-1b-v2\"\n    base_url:\
      \ http://llama-3.2-nv-rerankqa:8000/v1\n\nnotification:\n  enable: true\n  endpoint:\
      \ \"http://host.docker.internal:60000/via-alert-callback\"\n  llm:\n    model:\
      \ meta/llama-3.1-8b-instruct\n    base_url: http://llama-3-1-8b-instruct:8000/v1\n\
      \    max_tokens: 2048\n    temperature: 0.2\n    top_p: 0.7\n"
  server_config_quad_gpu:
    content: "summarization:\n  enable: true\n  method: \"batch\"\n  llm:\n    model:\
      \ \"meta/llama-3.1-70b-instruct\"\n    base_url: http://llama-3-1-70b-instruct:8000/v1\n\
      \    max_tokens: 2048\n    temperature: 0.2\n    top_p: 0.7\n  embedding:\n\
      \    model: \"nvidia/llama-3.2-nv-embedqa-1b-v2\"\n    base_url: http://llama-3-2-nv-embedqa:8000/v1\n\
      \  params:\n    batch_size: 5\n    batch_max_concurrency: 20\n  prompts:\n \
      \   caption: \"Write a concise and clear dense caption for the provided warehouse\
      \ video, focusing on irregular or hazardous events such as boxes falling, workers\
      \ not wearing PPE, workers falling, workers taking photographs, workers chitchatting,\
      \ forklift stuck, etc. Start and end each sentence with a time stamp.\"\n  \
      \  caption_summarization: \"You should summarize the following events of a warehouse\
      \ in the format start_time:end_time:caption. For start_time and end_time use\
      \ . to seperate seconds, minutes, hours. If during a time segment only regular\
      \ activities happen, then ignore them, else note any irregular activities in\
      \ detail. The output should be bullet points in the format start_time:end_time:\
      \ detailed_event_description. Don't return anything else except the bullet points.\"\
      \n    summary_aggregation: \"You are a warehouse monitoring system. Given the\
      \ caption in the form start_time:end_time: caption, Aggregate the following\
      \ captions in the format start_time:end_time:event_description. If the event_description\
      \ is the same as another event_description, aggregate the captions in the format\
      \ start_time1:end_time1,...,start_timek:end_timek:event_description. If any\
      \ two adjacent end_time1 and start_time2 is within a few tenths of a second,\
      \ merge the captions in the format start_time1:end_time2. The output should\
      \ only contain bullet points.  Cluster the output into Unsafe Behavior, Operational\
      \ Inefficiencies, Potential Equipment Damage and Unauthorized Personnel\"\n\n\
      chat:\n  rag: graph-rag # graph-rag or vector-rag #If using a small LLM model,\
      \ vector-rag is recommended.\n  params:\n    batch_size: 5\n    top_k: 5\n \
      \ llm:\n    model: \"meta/llama-3.1-70b-instruct\"\n    base_url: http://llama-3-1-70b-instruct:8000/v1\n\
      \    max_tokens: 2048\n    temperature: 0.2\n    top_p: 0.7\n  embedding:\n\
      \    model: \"nvidia/llama-3.2-nv-embedqa-1b-v2\"\n    base_url: http://llama-3-2-nv-embedqa:8000/v1\n\
      \  reranker:\n    model: \"nvidia/llama-3.2-nv-rerankqa-1b-v2\"\n    base_url:\
      \ http://llama-3.2-nv-rerankqa:8000/v1\n\nnotification:\n  enable: true\n  endpoint:\
      \ \"http://host.docker.internal:60000/via-alert-callback\"\n  llm:\n    model:\
      \ \"meta/llama-3.1-70b-instruct\"\n    base_url: http://llama-3-1-70b-instruct:8000/v1\n\
      \    max_tokens: 2048\n    temperature: 0.2\n    top_p: 0.7\n"
  server_config_single_gpu:
    content: "summarization:\n  enable: true\n  method: \"batch\"\n  llm:\n    model:\
      \ \"meta/llama-3.1-70b-instruct\"\n    base_url: \"https://integrate.api.nvidia.com/v1\"\
      \n    max_tokens: 2048\n    temperature: 0.2\n    top_p: 0.7\n  embedding:\n\
      \    model: \"nvidia/llama-3.2-nv-embedqa-1b-v2\"\n    base_url: \"https://integrate.api.nvidia.com/v1\"\
      \n  params:\n    batch_size: 5\n    batch_max_concurrency: 20\n  prompts:\n\
      \    caption: \"Write a concise and clear dense caption for the provided warehouse\
      \ video, focusing on irregular or hazardous events such as boxes falling, workers\
      \ not wearing PPE, workers falling, workers taking photographs, workers chitchatting,\
      \ forklift stuck, etc. Start and end each sentence with a time stamp.\"\n  \
      \  caption_summarization: \"You should summarize the following events of a warehouse\
      \ in the format start_time:end_time:caption. For start_time and end_time use\
      \ . to seperate seconds, minutes, hours. If during a time segment only regular\
      \ activities happen, then ignore them, else note any irregular activities in\
      \ detail. The output should be bullet points in the format start_time:end_time:\
      \ detailed_event_description. Don't return anything else except the bullet points.\"\
      \n    summary_aggregation: \"You are a warehouse monitoring system. Given the\
      \ caption in the form start_time:end_time: caption, Aggregate the following\
      \ captions in the format start_time:end_time:event_description. If the event_description\
      \ is the same as another event_description, aggregate the captions in the format\
      \ start_time1:end_time1,...,start_timek:end_timek:event_description. If any\
      \ two adjacent end_time1 and start_time2 is within a few tenths of a second,\
      \ merge the captions in the format start_time1:end_time2. The output should\
      \ only contain bullet points.  Cluster the output into Unsafe Behavior, Operational\
      \ Inefficiencies, Potential Equipment Damage and Unauthorized Personnel\"\n\n\
      chat:\n  rag: graph-rag # graph-rag or vector-rag #If using a small LLM model,\
      \ vector-rag is recommended.\n  params:\n    batch_size: 5\n    top_k: 5\n \
      \ llm:\n    model: \"meta/llama-3.1-70b-instruct\"\n    base_url: \"https://integrate.api.nvidia.com/v1\"\
      \n    max_tokens: 2048\n    temperature: 0.2\n    top_p: 0.7\n  embedding:\n\
      \    model: \"nvidia/llama-3.2-nv-embedqa-1b-v2\"\n    base_url: \"https://integrate.api.nvidia.com/v1\"\
      \n  reranker:\n    model: \"nvidia/llama-3.2-nv-rerankqa-1b-v2\"\n    base_url:\
      \ \"https://integrate.api.nvidia.com/v1\"\n\nnotification:\n  enable: true\n\
      \  endpoint: \"http://host.docker.internal:60000/via-alert-callback\"\n  llm:\n\
      \    model: \"meta/llama-3.1-70b-instruct\"\n    base_url: \"https://integrate.api.nvidia.com/v1\"\
      \n    max_tokens: 2048\n    temperature: 0.2\n    top_p: 0.7\n"
networks:
  devx:
    driver: bridge
services:
  devx:
    environment:
      COMPOSE_PROJECT_NAME: ${COMPOSE_PROJECT_NAME:-devx}
      DISABLE_GUARDRAILS: 'true'
      MODEL_PATH: git:https://huggingface.co/Efficient-Large-Model/NVILA-15B
      NGC_API_KEY: ${NGC_API_KEY}
      NVILA_VIDEO_MAX_TILES: '1'
      VIA_IMAGE: nvcr.io/nvidia/blueprint/vss-engine:2.3.0
      VLM_MODEL_TO_USE: nvila
    image: ghcr.io/rmkraus/test-workshop/devx:main
    ipc: host
    networks:
    - devx
    ports:
    - 8888:8888
    restart: always
    volumes:
    - ../test-workshop:/project:cached
    - /var/run/docker.sock:/var/run/docker.sock
    - devx_home:/home/nvidia
  graph-db:
    environment:
      NEO4J_AUTH: ${GRAPH_DB_USERNAME:-neo4j}/${GRAPH_DB_PASSWORD:-password}
      NEO4J_PLUGINS: '["apoc"]'
    healthcheck:
      interval: 1s
      retries: 20
      start_period: 3s
      test: wget http://localhost:7474 || exit 1
      timeout: 10s
    hostname: graph-db
    image: neo4j:5.26.4
    networks:
    - devx
    ports:
    - 7474:7474
    - 7687:7687
    restart: always
  llama-3-1-70b-instruct:
    deploy:
      resources:
        reservations:
          devices:
          - capabilities:
            - gpu
            device_ids:
            - '2'
            - '3'
            driver: nvidia
    environment:
    - NGC_API_KEY=${NGC_API_KEY}
    - NIM_LOW_MEMORY_MODE=1
    - NIM_RELAX_MEM_CONSTRAINTS=1
    extra_hosts:
      host.docker.internal: host-gateway
    healthcheck:
      interval: 30s
      retries: 60
      start_period: 1800s
      test:
      - CMD
      - python3
      - -c
      - import http.client; conn = http.client.HTTPConnection('localhost', 8000);
        conn.request('GET', '/v1/health/ready'); response = conn.getresponse(); exit(0
        if response.status == 200 else 1)
      timeout: 10s
    hostname: llama-3-1-70b-instruct
    image: nvcr.io/nim/meta/llama-3.1-70b-instruct:1.3.3
    networks:
    - devx
    ports:
    - 8007:8000
    profiles:
    - local-deployment-quad-gpu
    restart: always
    shm_size: 16gb
    volumes:
    - via-nim-cache:/opt/nim/.cache
  llama-3-1-8b-instruct:
    deploy:
      resources:
        reservations:
          devices:
          - capabilities:
            - gpu
            device_ids:
            - '1'
            driver: nvidia
    environment:
    - NGC_API_KEY=${NGC_API_KEY}
    - NIM_LOW_MEMORY_MODE=1
    - NIM_RELAX_MEM_CONSTRAINTS=1
    extra_hosts:
      host.docker.internal: host-gateway
    healthcheck:
      interval: 30s
      retries: 60
      start_period: 1800s
      test:
      - CMD
      - python3
      - -c
      - import http.client; conn = http.client.HTTPConnection('localhost', 8000);
        conn.request('GET', '/v1/health/ready'); response = conn.getresponse(); exit(0
        if response.status == 200 else 1)
      timeout: 10s
    hostname: llama-3-1-8b-instruct
    image: nvcr.io/nim/meta/llama-3.1-8b-instruct:1.3.3
    networks:
    - devx
    ports:
    - 8007:8000
    profiles:
    - local-deployment-dual-gpu
    restart: always
    shm_size: 16gb
    volumes:
    - via-nim-cache:/opt/nim/.cache
  llama-3-2-nv-embedqa:
    deploy:
      resources:
        reservations:
          devices:
          - capabilities:
            - gpu
            device_ids:
            - '1'
            driver: nvidia
    environment:
    - NGC_API_KEY=${NGC_API_KEY}
    - NIM_SERVER_PORT=8000
    - NIM_MODEL_PROFILE=f7391ddbcb95b2406853526b8e489fedf20083a2420563ca3e65358ff417b10f
    extra_hosts:
      host.docker.internal: host-gateway
    healthcheck:
      interval: 30s
      retries: 60
      start_period: 1800s
      test:
      - CMD
      - curl
      - -f
      - http://localhost:8000/v1/health/ready
      timeout: 10s
    hostname: llama-3-2-nv-embedqa
    image: nvcr.io/nim/nvidia/llama-3.2-nv-embedqa-1b-v2:1.3.0
    networks:
    - devx
    ports:
    - 8006:8000
    profiles:
    - local-deployment-dual-gpu
    - local-deployment-quad-gpu
    restart: always
    shm_size: 16gb
    volumes:
    - via-nim-cache:/opt/nim/.cache
  llama-3.2-nv-rerankqa:
    deploy:
      resources:
        reservations:
          devices:
          - capabilities:
            - gpu
            device_ids:
            - '1'
            driver: nvidia
    environment:
    - NGC_API_KEY=${NGC_API_KEY}
    - NIM_SERVER_PORT=8000
    - NIM_MODEL_PROFILE=f7391ddbcb95b2406853526b8e489fedf20083a2420563ca3e65358ff417b10f
    extra_hosts:
      host.docker.internal: host-gateway
    healthcheck:
      interval: 30s
      retries: 60
      start_period: 1800s
      test:
      - CMD
      - curl
      - -f
      - http://localhost:8000/v1/health/ready
      timeout: 10s
    hostname: llama-3.2-nv-rerankqa
    image: nvcr.io/nim/nvidia/llama-3.2-nv-rerankqa-1b-v2:1.3.0
    networks:
    - devx
    ports:
    - 8005:8000
    profiles:
    - local-deployment-dual-gpu
    - local-deployment-quad-gpu
    restart: always
    shm_size: 16gb
    volumes:
    - via-nim-cache:/opt/nim/.cache
  via-server-dual-gpu:
    configs:
    - source: server_config_dual_gpu
      target: /opt/nvidia/via/default_config.yaml
    depends_on: &id001
    - graph-db
    deploy: &id002
      resources:
        reservations:
          devices:
          - capabilities:
            - gpu
            device_ids:
            - '0'
            driver: nvidia
    environment: &id003
      AZURE_OPENAI_API_KEY: ${AZURE_OPENAI_API_KEY:-}
      AZURE_OPENAI_ENDPOINT: ${AZURE_OPENAI_ENDPOINT:-}
      BACKEND_PORT: '8100'
      CA_RAG_EMBEDDINGS_DIMENSION: '500'
      DISABLE_CA_RAG: ${DISABLE_CA_RAG:-false}
      DISABLE_CV_PIPELINE: ${DISABLE_CV_PIPELINE:-true}
      DISABLE_FRONTEND: ${DISABLE_FRONTEND:-false}
      DISABLE_GUARDRAILS: ${DISABLE_GUARDRAILS:-false}
      ENABLE_AUDIO: ${ENABLE_AUDIO:-false}
      ENABLE_DENSE_CAPTION: ${ENABLE_DENSE_CAPTION:-}
      ENABLE_RIVA_SERVER_READINESS_CHECK: ${ENABLE_RIVA_SERVER_READINESS_CHECK:-}
      FORCE_CA_RAG_RESET: ${FORCE_CA_RAG_RESET:-}
      FORCE_SW_AV1_DECODER: ${FORCE_SW_AV1_DECODER:-}
      FRONTEND_PORT: '9100'
      GDINO_INFERENCE_INTERVAL: ${GDINO_INFERENCE_INTERVAL:-}
      GDINO_MODEL_PATH: ${GDINO_MODEL_PATH:-}
      GRAPH_DB_PASSWORD: ${GRAPH_DB_PASSWORD:-password}
      GRAPH_DB_URI: ${GRAPH_DB_URI:-bolt://graph-db:7687}
      GRAPH_DB_USERNAME: ${GRAPH_DB_USERNAME:-neo4j}
      GRAPH_RAG_PROMPT_CONFIG: ${GRAPH_RAG_PROMPT_CONFIG:-}
      INSTALL_PROPRIETARY_CODECS: ${INSTALL_PROPRIETARY_CODECS:-false}
      MILVUS_DB_HOST: ${MILVUS_DB_HOST:-}
      MILVUS_DB_PORT: ${MILVUS_DB_PORT:-}
      MODEL_PATH: ${MODEL_PATH:-}
      NGC_API_KEY: ${NGC_API_KEY:-}
      NUM_CV_CHUNKS_PER_GPU: ${NUM_CV_CHUNKS_PER_GPU:-}
      NUM_GPUS: ${NUM_GPUS:-}
      NUM_VLM_PROCS: ${NUM_VLM_PROCS:-}
      NVIDIA_API_KEY: ${NGC_API_KEY:-}
      NVIDIA_VISIBLE_DEVICES: ${NVIDIA_VISIBLE_DEVICES:-all}
      NVILA_VIDEO_MAX_TILES: ${NVILA_VIDEO_MAX_TILES:-}
      NV_LLMG_CLIENT_ID: ${NV_LLMG_CLIENT_ID:-}
      NV_LLMG_CLIENT_SECRET: ${NV_LLMG_CLIENT_SECRET:-}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      OPENAI_API_VERSION: ${OPENAI_API_VERSION:-}
      RIVA_ASR_GRPC_PORT: ${RIVA_ASR_GRPC_PORT:-50051}
      RIVA_ASR_HTTP_PORT: ${RIVA_ASR_HTTP_PORT:-}
      RIVA_ASR_MODEL_NAME: ${RIVA_ASR_MODEL_NAME:-}
      RIVA_ASR_SERVER_API_KEY: ${RIVA_ASR_SERVER_API_KEY:-}
      RIVA_ASR_SERVER_FUNC_ID: ${RIVA_ASR_SERVER_FUNC_ID:-}
      RIVA_ASR_SERVER_IS_NIM: ${RIVA_ASR_SERVER_IS_NIM:-true}
      RIVA_ASR_SERVER_URI: ${RIVA_ASR_SERVER_URI:-parakeet-ctc-asr}
      RIVA_ASR_SERVER_USE_SSL: ${RIVA_ASR_SERVER_USE_SSL:-false}
      TRT_ENGINE_PATH: ${TRT_ENGINE_PATH:-}
      TRT_LLM_MEM_USAGE_FRACTION: ${TRT_LLM_MEM_USAGE_FRACTION:-}
      TRT_LLM_MODE: ${TRT_LLM_MODE:-}
      VIA_VLM_API_KEY: ${VIA_VLM_API_KEY:-}
      VIA_VLM_OPENAI_MODEL_DEPLOYMENT_NAME: ${VIA_VLM_OPENAI_MODEL_DEPLOYMENT_NAME:-}
      VILA_ENGINE_NGC_RESOURCE: ${VILA_ENGINE_NGC_RESOURCE:-}
      VILA_FORCE_ENGINE_BUILD: ${VILA_FORCE_ENGINE_BUILD:-false}
      VILA_LORA_PATH: ${VILA_LORA_PATH:-}
      VLM_BATCH_SIZE: ${VLM_BATCH_SIZE:-}
      VLM_INPUT_HEIGHT: ${VLM_INPUT_HEIGHT:-}
      VLM_INPUT_WIDTH: ${VLM_INPUT_WIDTH:-}
      VLM_MODEL_TO_USE: ${VLM_MODEL_TO_USE:-openai-compat}
      VSS_EXTRA_ARGS: ${VSS_EXTRA_ARGS:-}
      VSS_LOG_LEVEL: ${VSS_LOG_LEVEL:-}
      VSS_RTSP_LATENCY: ${VSS_RTSP_LATENCY:-}
      VSS_RTSP_TIMEOUT: ${VSS_RTSP_TIMEOUT:-}
    extra_hosts: &id004
      host.docker.internal: host-gateway
    healthcheck: &id005
      interval: 10s
      retries: 30
      start_period: 300s
      test:
      - CMD
      - sh
      - -c
      - curl -f http://localhost:8100/health/ready
      timeout: 5s
    hostname: via-server
    image: ${VIA_IMAGE:-nvcr.io/nvidia/blueprint/vss-engine:2.3.0}
    ipc: host
    networks: &id006
    - devx
    ports: &id007
    - 8100:8100
    - 9100:9100
    profiles:
    - local-deployment-dual-gpu
    restart: always
    runtime: nvidia
    stdin_open: true
    tty: true
    ulimits: &id008
      memlock:
        hard: -1
        soft: -1
      stack: 67108864
    volumes: &id009
    - via-ngc-model-cache:/root/.via/ngc_model_cache
    - via-hf-cache:/tmp/huggingface
  via-server-quad-gpu:
    configs:
    - source: server_config_quad_gpu
      target: /opt/nvidia/via/default_config.yaml
    depends_on: *id001
    deploy: *id002
    environment: *id003
    extra_hosts: *id004
    healthcheck: *id005
    hostname: via-server
    image: ${VIA_IMAGE:-nvcr.io/nvidia/blueprint/vss-engine:2.3.0}
    ipc: host
    networks: *id006
    ports: *id007
    profiles:
    - local-deployment-quad-gpu
    restart: always
    runtime: nvidia
    stdin_open: true
    tty: true
    ulimits: *id008
    volumes: *id009
  via-server-single-gpu:
    configs:
    - source: server_config_single_gpu
      target: /opt/nvidia/via/default_config.yaml
    depends_on: *id001
    deploy: *id002
    environment: *id003
    extra_hosts: *id004
    healthcheck: *id005
    hostname: via-server
    image: ${VIA_IMAGE:-nvcr.io/nvidia/blueprint/vss-engine:2.3.0}
    ipc: host
    networks: *id006
    ports: *id007
    profiles:
    - local-deployment-single-gpu
    restart: always
    runtime: nvidia
    stdin_open: true
    tty: true
    ulimits: *id008
    volumes: *id009
volumes:
  devx_home: null
  via-hf-cache: null
  via-ngc-model-cache: null
  via-nim-cache: null
