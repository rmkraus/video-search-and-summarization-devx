# Start the VSS Services

<img src="_static/robots/startup.png" alt="VSS Robot Character" style="float:right; max-width:350px;margin:15px;" />


Before getting started, we will use Docker Compose to start the Blueprint.

To do this, you will need an NVIDIA API Key. If you don't already have one, you can [generate an API Key](https://build.nvidia.com/settings/api-keys). This will begin with `nvapi-.....`.

The setup will be done on the command line. To get started, open a new <button onclick="openNewTerminal();"><i class="fas fa-terminal"></i> terminal</button>.

<!--fold:break -->

## Download Data Files

Let's first make sure we have downloaded the data files we will need. The following command will download any required files.

```bash
git lfs pull
```

<!--fold:break -->

## Login to the NGC Registry

1. Log into NVIDIA's NGC Registry. Use this command and enter your API Key when prompted.

    ```bash
    sudo docker login -u '$oauthtoken' nvcr.io
    ```

1. Save your API Key in an environment variable.

    ```bash
    export NGC_API_KEY="nvapi-........"
    ```

<!--fold:break -->

## Start the Blueprint services

Now, we will start the VSS Blueprint. Note that these commands will take some time to return as they download and stage the necessary models and code.

A few profiles have been made available depending on your hardware.

{{#isBrev}}
> **Note:** Looks like you are running in Brev! We recommend using the Dual GPU profile for optimal performance.
{{/isBrev}}

<!-- tabs:start -->

### **Single GPU - min 80GB**

|  | Model | Self-hosted | GPU ID |
| --- | --- | --- | -- |
| VLM | nvila-15b | 游릭 yes | 1 |
| Embedding | nvidia/llama-3.2-nv-embedqa-1b-v2 | 游댮 no | |
| Reranking | nvidia/llama-3.2-nv-rerankqa-1b-v2" | 游댮 no |
| LLM | meta/llama-3.1-70b-instruct | 游댮 no | |

To start the Blueprint in this configuration, use the following command:

```bash
sudo -E docker compose --profile local-deployment-single-gpu up -d
```

Watch the status of the containers and wait for them to become healthy with this command:

```bash
watch sudo -E docker compose --profile local-deployment-single-gpu ps
```

### **Dual GPUs - min 80GB**

|  | Model | Self-hosted | GPU ID |
| --- | --- | --- | --- |
| VLM | nvila-15b | 游릭 yes | 1 |
| Embedding | nvidia/llama-3.2-nv-embedqa-1b-v2 | 游릭 yes | 2 |
| Reranking | nvidia/llama-3.2-nv-rerankqa-1b-v2 | 游릭 yes | 2 |
| LLM | meta/llama-3.1-8b-instruct | 游릭 yes | 2 |


To start the Blueprint in this configuration, use the following command:

```bash
sudo -E docker compose --profile local-deployment-dual-gpu up -d
```

Watch the status of the containers and wait for them to become healthy with this command:

```bash
watch sudo -E docker compose --profile local-deployment-dual-gpu ps
```

### **Quad GPUs - min 40GB**

|  | Model | Self-hosted | GPU ID |
| --- | --- | --- | --- |
| VLM | nvila-15b | 游릭 yes | 1 |
| Embedding | nvidia/llama-3.2-nv-embedqa-1b-v2 | 游릭 yes | 2 |
| Reranking | nvidia/llama-3.2-nv-rerankqa-1b-v2 | 游릭 yes | 2 |
| LLM | meta/llama-3.1-70b-instruct | 游릭 yes | 3 |


To start the Blueprint in this configuration, use the following command:

```bash
sudo -E docker compose --profile local-deployment-quad-gpu up -d
```

Watch the status of the containers and wait for them to become healthy with this command:

```bash
watch sudo -E docker compose --profile local-deployment-quad-gpu ps
```

<!-- tabs:end -->

Once all of the services have a status of `Up` and `healthy`, you are ready to continue.

<!--fold:break -->

## Stopping the Blueprint

Don't run these commands now! But these commands will help you shutdown the Blueprint, when you are ready.

<!-- tabs:start -->

### **Single GPU - min 40GB**

```bash
sudo -E docker compose --profile local-deployment-single-gpu down
```

### **Dual GPUs - min 40GB**

```bash
sudo -E docker compose --profile local-deployment-dual-gpu down
```

### **Quad GPUs - min 80GB**

```bash
sudo -E docker compose --profile local-deployment-quad-gpu down
```

<!-- tabs:end -->